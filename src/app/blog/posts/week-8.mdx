---
title: "Week #8 - Start of the Project"
publishedAt: "2025-03-16"
summary: "Start of the Project"
tag: "progress"
---

## Key Takeaways from Lecture

### Normalization
- **1NF:** Atomic values in each column.
- **2NF:** No partial dependencies on candidate keys.
- **3NF:** No transitive dependencies.
- **BCNF:** Every determinant is a candidate key.
- **4NF:** Eliminate multi-valued dependencies.
- **5NF:** Join dependencies implied by candidate keys.
- **Lossless Decomposition:** No information loss when splitting tables.

### Debugging
- **Process:** Reproduce → Locate → Identify → Fix → Test → Document.
- **Strategies:** Brute Force, Backtracking, Forward Analysis, Cause Elimination, Static/Dynamic Analysis, Collaborative Debugging, Logging, Automated Debugging.
- **Common Errors:** Syntax, Logical, Runtime, Stack Overflow, Infinite Loops, Concurrency Issues.
- **Tools:** IDEs (Visual Studio, Eclipse), Standalone Debuggers (GDB), Logging Utilities, Static/Dynamic Analysis Tools, Performance Profilers.
- **Testing vs Debugging:** Testing finds bugs; debugging fixes them.
- **Defensive Programming:** Handle exceptions and edge cases.

### General Tips
- **Normalization:** Aim for at least 3NF for clean, efficient databases.
- **Debugging:** Start small, use print statements, and leverage debugging tools.

## This week's work

I aimed to understand the overall theorical knowledge needed to understand how to build this project.
The RL concepts needed, libraries commonly used to formulate them, etc. 
I also wanted to gather data from Dubai's most congested highways, like Sheikh Mohammed Road, which I frequent daily, spending over 2 hours one-way.

I also got started with the report, and will be updating it daily as I learn more about this project & progress with the implementation. To begin with, I went through the provided samples and made a template with the content relevant to the project. I wrote some of the sections, like abstract, introduction, etc. We were told to follow IEEE style referencing, so added the papers I've referenced so far as well.  

Some notes I made while learning:
- Q-Learning gives the probability of how good an action is, given a state, ie. how big the reward is asa result of it. The action with the highest rewards is chosen. However, the policy is not explicitly learned; it’s implicit in the Q-values.
- Policy Learning gives the probability of selecting an action, given a state, ie. maps states to actions directly without computing each reward. 

- When to use which: 
    - Q-learning is great for discrete action spaces and when you need precise value estimates.
    - Policy learning is more flexible and can handle continuous action spaces or complex policies (e.g., neural networks).


Referenced Papers & Videos:
- [A friendly introduction to deep reinforcement learning, Q-networks and policy gradients](https://www.youtube.com/watch?v=SgC6AZss478), Serrano Academy
- [Policies and Value Functions - Good Actions for a Reinforcement Learning Agent](https://www.youtube.com/watch?v=eMxOGwbdqKY), DeepLizard
- [Reinforcement Learning: on-policy vs off-policy algorithms](https://www.youtube.com/watch?v=YUKUXoUg3Nc), CodeEmperium
- [Soft Actor-Critic: a beginner-friendly introduction](https://www.youtube.com/watch?v=LN29DDlHp1U), Youtube
- [Soft Actor-Critic | Lecture 83 (Part 3) | Applied Deep Learning](https://www.youtube.com/watch?v=rXuAyHQghKw), CU Boulder
- [Traffic APIs | TomTom](https://www.tomtom.com/products/traffic-apis/)

## Student Tracker Submission
We were told to fill a form regarding how satisfied we were with our progress and how our experiences with our professors have been so far.
I outlined how supportive Ms. Geethu has been and how she gave crucial advice that has lead to a great project, but also about how I'm dissappointed in myself for my delay in finalising my project topic.
I promised to catch up and live up to the expectations of my professors & peers. 
