---
title: "Week #16 - Completing Model Design"
publishedAt: "2025-05-11"
summary: "Completing Model Design"
tag: "progress"
---

Last time I focused on a general actor-critic design. This time, I added modifications & new parts
while referring to an SAC video. Fixed issues with previous code - divided outputs into separate functions,
since learning for each is different. 

Replay buffer keeps track of episodes - their initial states, output & result. Code below: 

<CodeBlock className="my-24"
    compact={false}
    codeInstances={[
        {
            code: `import numpy as np

class ReplayBuffer:
    def __init__(self, max_size, input_shape): #, n_actions
        self.mem_size = max_size
        self.mem_cntr = 0
        self.state_mem = np.zeros((self.mem_size, *input_shape))
        self.n_state_mem = np.zeros((self.mem_size, *input_shape))
        self.action_mem = np.zeros((self.mem_size))  #, n_actions
        self.reward_mem = np.zeros(self.mem_size)
        self.terminal_mem = np.zeros(self.mem_size, dtype=bool)
    
    def store_transition(self, state, action, reward, n_state, done):
        index = self.mem_cntr % self.mem_size
        self.state_mem[index] = state
        self.n_state_mem[index] = n_state
        self.reward_mem[index] = reward
        self.action_mem[index] = action
        self.terminal_mem[index] = done
        self.mem_cntr += 1
    
    def sample_buffer(self, batch_size):
        max_mem = min(self.mem_cntr, self.mem_size)
        batch = np.random.choice(max_mem, batch_size)
        states = self.state_mem[batch]
        n_states = self.n_state_mem[batch]
        actions = self.action_mem[batch]
        rewards = self.reward_mem[batch]
        dones = self.terminal_mem[batch]
        return states, actions, rewards, states, dones`,
            label: 'replay_buffer.py',
            language: 'python',
            showLineNumbers: true
        },
    ]}
    copyButton
    maxHeight="400px"
/>


Below code is for the main learning function, addressing each part individually:

<CodeBlock className="my-24"
    compact={false}
    codeInstances={[
        {
            code: `def learn(self):
        if self.memory.mem_cntr < self.batch_size:
            return

        # 1. Sample a batch
        state, action, reward, next_state, done = self.memory.sample_buffer(self.batch_size)

        state = tf.convert_to_tensor(state, dtype=tf.float32)
        next_state = tf.convert_to_tensor(next_state, dtype=tf.float32)
        reward = tf.convert_to_tensor(reward, dtype=tf.float32)
        action = tf.convert_to_tensor(action, dtype=tf.float32)
        done = tf.convert_to_tensor(done, dtype=tf.float32)

        # Update Value Network 
        with tf.GradientTape() as tape:
            value = tf.squeeze(self.ac.get_value(state), 1)

            # Sample from current policy
            new_action, log_prob = self.ac.sample_normal(state)
            q_value = tf.squeeze(self.ac.get_q(tf.concat([state, new_action], axis=1)), 1)

            # Target: Q - log_pi
            value_target = q_value - tf.squeeze(log_prob, 1)
            value_loss = 0.5 * tf.keras.losses.MSE(value, value_target)

        value_grads = tape.gradient(value_loss, self.ac.trainable_variables)
        self.ac.optimizer.apply_gradients(zip(value_grads, self.ac.trainable_variables))

        # Update Actor Network
        with tf.GradientTape() as tape:
            new_action, log_prob = self.ac.sample_normal(state)
            q_value = tf.squeeze(self.ac.get_q(tf.concat([state, new_action], axis=1)), 1)

            actor_loss = tf.reduce_mean(log_prob - q_value)

        actor_grads = tape.gradient(actor_loss, self.ac.trainable_variables)
        self.ac.optimizer.apply_gradients(zip(actor_grads, self.ac.trainable_variables))

        # Update Critic / Q-function
        with tf.GradientTape() as tape:
            # Compute Q target: reward + Î³ * V(next_state)
            value_ = tf.squeeze(self.ac.get_value(next_state), 1)
            q_target = reward + self.gamma * value_ * (1 - done)

            q_pred = tf.squeeze(self.ac.get_q(tf.concat([state, action], axis=1)), 1)
            critic_loss = 0.5 * tf.keras.losses.MSE(q_pred, q_target)

        critic_grads = tape.gradient(critic_loss, self.ac.trainable_variables)
        self.ac.optimizer.apply_gradients(zip(critic_grads, self.ac.trainable_variables))`,
            label: 'agent.py',
            language: 'python',
            showLineNumbers: true
        },
    ]}
    copyButton
    maxHeight="400px"
/>
