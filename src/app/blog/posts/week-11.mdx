---
title: "Week #11 - Deferral"
publishedAt: "2025-04-06"
summary: "Deferral"
tag: "progress"
---

No class this week due to Eid holidays.

## Update regarding deadline

The standard deadline was April 11th, that is the end of next week. But as you must've noticed so far, the project progress is not going how it should be.
I had personal reasons for this, but I waited this far to try and catch up. But now that there is little time and a lot of work, I decided to talk to the module & CEI department head Dr. Fehmida about it.
She has been aware of my situation, and had approved my gap year 2 years ago as well for the same reasons. I explained the current circumstances. 
She was very understanding, and advised me to go ahead and apply for a defferal. That would mean the deadline would be extended till mid-May.
I also discussed it with my supervisor, and submitted the application. I feel relieved about the extra time, and am thankful to them for the consideration. 

## This week's work
 
# Data
Checked the TomTom API documentation in detail. Here are the detailed API endpoints relevant to this project:
-	Traffic API – Real-time
-	Flow Segment Data – Provides current speed, also free flow speed, current & free flow travel times, and shape of road segment.
-	Vector Flow Tiles – Includes traffic level between 0 & 1.
-	Incident Details – Describes incidents on bounded area requested.
-	Traffic Stats API - Historical, returns data over date & time range
-	Traffic Density Service – Provides density, required to represent current traffic. Also gives static speed limit, and length & shape of road segment.
-	Area analysis Service – Details of the segment, like overall average speed & travel time, and their standard deviations. This could be useful to bound the speed limits.
-	Route Analysis Service – Data over a series of points. Useful in case segments made by TomTom itself are not suitable. 
-	Maps API – Helps with visualization.

# Algorithm Theory
I understand the concept of actor-critic to some extent, but don't fully know the exact differences between normal actor-critic and SAC. So I studied about that theoretically a bit more, and updated the report with my understanding. 

# Code
After coding the basic actor-critic & going through some theory, I started to modify my code according to an SAC tutorial. 
Main changes included adding a 'replay buffer', that stores experiences as the agent does actions & observes results. 
The tutorial showed seperate networks, but I will stick with one, due to straight-forward data. I chose NNs since it is dynamically changing, but also don't want to make it too complex.
The SAC paper had more complex data, so I wanted my approach to be simpler, but carry the pros of the method.  

# References
- [Can a Random Reinforcement Learning Agent Maximize its Score? Soft Actor Critic (SAC) in Tensorflow2](https://www.youtube.com/watch?v=YKhkTOU0l20)
- [Replay Memory Explained - Experience for Deep Q-Network Training](https://www.youtube.com/watch?v=Bcuj2fTH4_4)